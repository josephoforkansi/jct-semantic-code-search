# config/config.yaml - JCT Semantic Code Search Configuration

# Embedding settings
embedding:
  model_name: "all-MiniLM-L6-v2"          # Lightweight default (~80MB, good for CPU)
  # Alternatives (better code understanding but heavier):
  # - "jinaai/jina-embeddings-v2-base-code"
  # - "microsoft/codebert-base"
  device: "cpu"                           # "cuda" if you have GPU
  batch_size: 32                          # For encoding multiple chunks

# Chunking settings (how to split code files)
chunking:
  strategy: "ast"                         # Options: "ast" (Python AST), "tree-sitter", "naive"
  max_chunk_size: 2000                    # Characters per chunk (VectorCode-inspired default ~2500)
  overlap_ratio: 0.2                      # Fraction of overlap between chunks (prevents info loss)
  min_chunk_size: 200                     # Ignore tiny fragments

# Vector storage / database
storage:
  type: "chroma"                          # "chroma" (persistent), "faiss" (in-memory/fast), "inmemory"
  collection_name: "jct_codebase"         # Unique per project if multi-repo
  persist_directory: "./.jct_vector_db"   # Where Chroma saves data (gitignored!)
  distance_metric: "cosine"               # Or "l2" / Euclidean

# Indexing behavior
indexing:
  respect_gitignore: true                 # Skip files in .gitignore
  include_patterns:                       # Glob patterns to include
    - "*.py"
  exclude_patterns:                       # Additional excludes
    - "__pycache__/*"
    - "*.pyc"
    - "venv/*"

# Search defaults
search:
  top_k: 5                                # Default number of results
  min_similarity_threshold: 0.6           # Ignore very low matches (0-1 scale)

# Logging / debug
logging:
  level: "INFO"                           # DEBUG for verbose output